{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b75af44",
   "metadata": {},
   "source": [
    "# Build your `Deep Convolutional Generative Adversarial Network (DCGAN)` for Deep Learning on Handwritten Digits `Image generation` Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d37633",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052d1878",
   "metadata": {},
   "source": [
    "\n",
    "## Annotations in cell of this Notebook: \n",
    " \n",
    "<font color='red'>Read carefully the Annotations below to understand what to write in the Notebook and what to answer in the Course Shell.</font>\n",
    " \n",
    " `##` is the python comments for assignment requirements and/or descriptions to the code below.\n",
    "\n",
    "`???` is the marker of the place that you should REPLACE with your own python code (either name of function[s], name of variable[s], expression[s], or a whole code line etc.), which should be able to be executed without errors. <strong>You may also find some inspirations from previous HomeWork assignment to write desired code for similar purpose/functionality </strong>\n",
    "\n",
    "`##-Qx: ` is the numbered identifier to indicate the question you should answer in the Course Shell for this homework. For example    `##-Q1: ` and `##-Q6: ` .   \n",
    "\n",
    "[`clickable link`]() is some tip/hint resources you many refer to in addition to your learning resources from this course or your self explorations.  \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea8e83c",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b09eec",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be604776",
   "metadata": {},
   "source": [
    "## Task Overview \n",
    "<img src=\"https://miro.medium.com/max/1400/1*N3nT9AXVnsFBta2R1eEMjg.png\" width=\"500\">\n",
    "　　　　　　　　　　　　　　　　　　　　　　　　　　　　　<strong>A Task Schema Diagram</strong>\n",
    "                                              \n",
    "Generative Adversarial Networks (GANs) is a framework to train generative models, such as deep convolutional neural networks for generating images. GAN composed of two models which are trained simultaneously by an adversarial process. A generator model (\"the artist\") learns to create fake images that look real, while a discriminator model (\"the art critic\") learns to tell real images apart from fakes. \n",
    "\n",
    "During training, the generator progressively becomes better at creating images that look real, while the discriminator becomes better at telling them apart. The process reaches equilibrium when the discriminator can hardly distinguish real images from fakes.\n",
    "\n",
    "For a simple GAN the generator is build as a simple fully connected network. The generator of a Deep Convolutional Generative Adversarial Network (DCGAN) are mainly composes of convolution layers without max pooling with convolutional stride and transposed convolution (Fractionally-strided convolution or Deconvolution) for the downsampling and the upsampling of 2D images.  \n",
    " \n",
    "\n",
    "You will conduct a `Image generation` task, by `building` a DCGAN architecture with Python to generate `fake images` from random `noise input`, that after proper `training` of the model it would look like images of `real handwritten digits`. \n",
    "\n",
    "The `TensorFlow 2.x` platform and its integrated version of `Keras API` (versus the traditional Keras Package) should be the handy tools for you to complete this task. Please take advantage of the official well supported TensorFlow & Keras [`Help Documentations`](https://www.tensorflow.org/api_docs/python/tf/keras) for correct usages of the API and functions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1273219",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "62f271a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import related Modules for TensorFlow framework and Keras API\n",
    "   \n",
    "\n",
    "## import the TensorFlow and give it alias \"tf\" \n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "## import the `Keras` API from TensorFlow \n",
    "from tensorflow import keras\n",
    "\n",
    "##-Q1: copy your composed code in the line above to answer corresponding question in the Course Shell\n",
    "\n",
    "\n",
    "## import the namespace \"layers\" and \"optimizer\" of Keras from Tensorflow \n",
    "##   - use comma and exactly one space to seperate each namespace\n",
    "# from tensorflow.keras import layers, optimizer\n",
    "from keras import layers, optimizers\n",
    "##-Q2: copy your composed code in the line above to answer corresponding question in the Course Shell\n",
    "\n",
    "\n",
    "## import the Numpy package and give it alias \"np\" \n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a5b384bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCGAN Project starts at 2023-06-02 15:55:07.517810\n"
     ]
    }
   ],
   "source": [
    "## Record the starting time of runing DCGAN project\n",
    "## Create a image folder to store generated fake images for every interval during training\n",
    "import os\n",
    "from datetime import datetime\n",
    "datetime_now = datetime.now()\n",
    "print(f\"DCGAN Project starts at {datetime_now}\")\n",
    "datetime_now = datetime_now.strftime(\"%Y%d%m_%H%M%S\")\n",
    "if os.path.exists('dcgan_fake_images'):       \n",
    "    os.rename(\"dcgan_fake_images\", f\"dcgan_fake_images_{datetime_now}\")    \n",
    "    os.makedirs('dcgan_fake_images')\n",
    "else:        \n",
    "    os.makedirs('dcgan_fake_images')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c187cba1",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56257cdb",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f9a181",
   "metadata": {},
   "source": [
    "## Prepare Dataset \n",
    "\n",
    "### The Image Dataset of MNIST Handwritten Digits - Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d279f533",
   "metadata": {},
   "source": [
    "The [`MNIST`](http://yann.lecun.com/exdb/mnist/) database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is widely used for training and testing Deep Learning Models.\n",
    "\n",
    "The images in the dataset contains small square images with pixels in grayscale, and each image present a real handwritten single digits between 0 and 9.  \n",
    "\n",
    "The image genration task is to generate fake images out of noise inputs and have the fake images looks like a handwritten digit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5eb975",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "Load the MNIST dataset distributed with Keras.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "67ea7b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the MNIST data from the Keras in Tensorflow framework\n",
    "mnist_data = keras.datasets.mnist\n",
    "  \n",
    "## We will only use the images to train the DCGAN, so use '_' to replace other variable locations\n",
    "##   Since we are not trying to train model to classify into correct labels \n",
    "##   We only need to identify if a image is real or fake as 1 or 0\n",
    "## replace ONLY the '???' in the code line below:\n",
    "(images_train, _),(_, _) = mnist_data.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa082396",
   "metadata": {},
   "source": [
    "###  Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f2a92195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Types:   uint8\n",
      "Shape / Dimensions of Data : (60000, 28, 28)\n",
      "Min: 0 \n",
      " Max:  255\n"
     ]
    }
   ],
   "source": [
    "## Find out the datatype, dimensions, and min/max value (i.e. value range) of the loaded data `images_train` \n",
    "## Feel free to explore all aspects that could help you understand the data\n",
    "\n",
    "\n",
    "print(\"Data Types:  \", images_train.dtype)\n",
    "print(\"Shape / Dimensions of Data :\", images_train.shape)\n",
    "print(\"Min:\", np.min(images_train), \"\\n Max: \", np.max(images_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ca6ede8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a function for visualizing some images\n",
    "def images_show(images, dim=(1,6), figsize=(12,6), num_postfix=-1):\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(images.shape[0]):\n",
    "        plt.subplot(dim[0], dim[1], i+1)\n",
    "        plt.imshow(images[i], cmap='Greys_r', interpolation='nearest')\n",
    "        plt.axis('off') \n",
    "    plt.savefig(f'dcgan_fake_images/fake_images_iter_{num_postfix:05d}.png') \n",
    "    plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f7108925",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Call the function images_show to show first 6 images in the loaded data, i.e. images_train\n",
    "\n",
    "loaded_image_samples = images_show(images_train[:5])\n",
    "loaded_image_samples  \n",
    "\n",
    "##-Q3: What are the code you should put in place of the ??? in the above code line to fulfill its purpose of design (requirements in the code comment)? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bdb9cf",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d0d6cd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To faciliate the model training, we would preprocess to normalize and reshape the images\n",
    "\n",
    "def images_preprocess(images):\n",
    "    ## Normalize the input images to the range of [-1, 1] \n",
    "    images = images / 127.5 - 1\n",
    "    \n",
    "    ## Convert dimension of all grayscale images to be enforce to have a color channel in its dimensions\n",
    "    ##   i.e. convert origianl 3D tensor (number_of_data, image_height, image_width) of `images_train`\n",
    "    ##        to 4D tensor by adding the addiontal dimension as the color channel\n",
    "    ##   - fill in only the number values to replace the ??? below:\n",
    "\n",
    "    # I'm assuming color channel here means grayscale so 1, but if not then make the 1 a 3. \n",
    "    images = images.reshape(images.shape[0], images.shape[1], images.shape[2], 1).astype('float32')   \n",
    "\n",
    "    ##-Q4: What are the number values you should put in place of the ??? in the above code line to fulfill its purpose of design (requirements in the comment)? \n",
    "    \n",
    "    return images\n",
    "\n",
    "## Use the function images_preprocess to preprocess the loaded data `images_train`  \n",
    "images_preprocessed = images_preprocess(images_train) \n",
    "##-Q5: What are the code you should put in place of the ??? in the above code line to fulfill its purpose of design (requirements in the comment)? \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00babcfe",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e5c339c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare dataset from the preprocessed image data by shuffle and batch \n",
    "\n",
    "## set parameter BUFFER_SIZE as the total number of images in the loaded data `images_train`\n",
    "\n",
    "BUFFER_SIZE = images_train.shape[0] # TODO: Check if this is right.\n",
    "    \n",
    "## set parameter BATCH_SIZE as 64\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "## create dataset for real images \n",
    "images_data = tf.data.Dataset.from_tensor_slices(images_preprocessed)\n",
    "def get_dataset_real(dataset, buffer=BUFFER_SIZE, batch=BATCH_SIZE, drop_remainder = True):\n",
    "    ## Shuffle and Batch to prepare the dataset for real images\n",
    "    dataset = dataset.shuffle(buffer).batch(batch, drop_remainder = drop_remainder).prefetch(1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "461a9beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create noises for generating fake images during the training stage\n",
    " \n",
    "## use a 1D random vector as the noise input \n",
    "##   for the generator model to create one fake image later \n",
    "## Set the dimension of the vector (i.e. NOISE_SIZE) as 100 \n",
    "##   i.e. the number of input \n",
    "NOISE_SIZE = 100\n",
    "\n",
    "## create a batch of random vectors that each is in the range of [-1, 1] (as noises)  \n",
    "##   i.e. random vectors for one batch\n",
    "def get_noise_one_batch(batch_size=BATCH_SIZE, noise_size=NOISE_SIZE):\n",
    "    noice_one_batch = np.random.uniform(-1, 1, (batch_size, noise_size))\n",
    "    return noice_one_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dd7612",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9021ee3",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdad99a",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecc9de6",
   "metadata": {},
   "source": [
    "## Build architecture of your DCGAN model\n",
    "\n",
    "DCGAN is based on GAN, which composed of two models in neural networks:\n",
    "\n",
    "- The first model is called a **Generator** and it aims to generate new data similar to the real one. In DCGAN it would generate an unexisted image that we call fake image for this project. \n",
    "\n",
    "- The second model is named the **Discriminator** and it aims to recognize if a data is ‘real’ — belongs to the original dataset; Or if it is ‘fake’ — generated by a forger. Whose output is a binary 1 or 0.\n",
    "\n",
    "\n",
    "\n",
    "The basic building block of neural network is the layer. \n",
    "You will configure each layer of your model, then compiling the model.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "For this task you are required to chain together layers as the architecture of the DCGAN. \n",
    "\n",
    "**Besides the layers you've already practiced and familiared with** in the **previous assignments** (E.g. **Dense, Convolutional, Flatten** layers etc.), \n",
    "\n",
    "the following  [**new types of layers**](https://www.tensorflow.org/api_docs/python/tf/keras/layers) might also needed in this task. Their API references listed below (as how to apply their corresponding **parameters** to configure the layer) might provide you more inspirations.\n",
    "\n",
    "- [Reshape](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Reshape)\n",
    "\n",
    "- [Conv2DTranspose](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose)\n",
    "\n",
    "- [LeakyReLU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LeakyReLU)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2824a726",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111c354a",
   "metadata": {},
   "source": [
    "### Build the Generator Model\n",
    "\n",
    "\n",
    "This Generator Model will receive noise vectors as inputs, and generated fake images as output;\n",
    "\n",
    "A vector in the inputs would be converted and upsampled several times until reaching the same size of a real image.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "00a7effd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build a Generator model by stacking keras layers sequentially  \n",
    " \n",
    "generator = keras.Sequential([        \n",
    "##-Q6: What is the function name that you should put in place of the ??? in the above code line to fulfill its purpose of design (requirements in the comment)? \n",
    "\n",
    "    ## *** READ *VERY CAREFULLY* Below What is DESCRIBED and REQUIRED to be FULFILLED *** \n",
    "    \n",
    "    ## Apply a fully-connected layer WITH bias, \n",
    "    ##   to receive and feature mapping the input, i.e. the noice vector\n",
    "    layers.Dense(7*7*128, input_shape=(NOISE_SIZE,), use_bias=True),   \n",
    "    ##-Q7: What are the code that you should put in place of the ??? in the above code line to fulfill its purpose of design (requirements in the comment)? \n",
    "    \n",
    "    ## Add a Reshape layer to change the output from previous 1D input layer into 3D \n",
    "    ##   which has 7X7 as size of generated 2D latent image \n",
    "    ##   with 128 features in the 3rd dimension\n",
    "    layers.Reshape([7, 7, 128]), \n",
    "    \n",
    "    ## Build a transposed convolution layer WITHOUT bias \n",
    "    ##      to do inversed convolution as a way to do learnable Upsampling;\n",
    "    ##   which has number of filters that equals to HALF of the \n",
    "    ##      feature dimension of the output from previous layer; \n",
    "    ##   5 X 5 as the kernel size;\n",
    "    ##   stride step as 2 for all of its spatial dimensions; and \n",
    "    ##   padding with zeros evenly to the surroundings of the input. \n",
    "    layers.Conv2DTranspose(3136, (5, 5), strides=(1, 1), padding='valid', use_bias=False),\n",
    "    ##-Q8: What are the code that you should put in place of the ??? in the above code line \n",
    "    ##     to fully fulfill its purpose of design (requirements in its above comments)? \n",
    "    \n",
    "    ##-Q9: After upsampling by transposed convolution layer with the required parameter configurations,\n",
    "    ##      What is the dimension of the 2D latent image that was 7X7 output from previous layer? \n",
    "    # 7X7X64\n",
    "       \n",
    "    \n",
    "    ## Add a Batch Normalization layer which would stabilize the learning procedure\n",
    "    ##   by standardizing activations from the prior layer to \n",
    "    ##   have normal distribution with the mean value of 0 and unit variance  \n",
    "    layers.BatchNormalization(),  \n",
    "    \n",
    "    ## Add a layer of Leaky ReLU activation function with alpha being 0.2, \n",
    "    ##   which still allow some negative value to pass through the network for continue learning\n",
    "    ##   even if it is negative, by multiply which with alpha     \n",
    "    layers.LeakyReLU(alpha=0.2), \n",
    "    \n",
    "    \n",
    "    \n",
    "    ## Build a transposed convolution layer WITHOUT bias;\n",
    "    ##   which has number of filters that equals to HALF of the \n",
    "    ##      feature dimension of the output from previous layer; \n",
    "    ##   3 X 3 as the kernel size; \n",
    "    ##   stride step as 2 for all of its spatial dimensions; and \n",
    "    ##   padding with zeros  evenlyto the surroundings of the input. \n",
    "    ## ???.???(???, (???, ???), strides=(???, ???), padding='???', use_bias=???),\n",
    "    layers.Conv2DTranspose(1568, (5, 5), strides=(2, 2), padding='valid', use_bias=False),\n",
    "    ##-Q10: What is the code line that you should put in place of the ??? line in the above  \n",
    "    ##     to fully fulfill its purpose of design (requirements in its above comments)? \n",
    "    ##   - use exactly only one space when needed, use only exactly one space after comma when comma is needed\n",
    "       \n",
    "    ##- Think about : After upsampling by this transposed convolution layer with the required parameter configurations,\n",
    "    ##      What is the dimension of the 2D latent image that was output from its previous layer? \n",
    "    # 7X7X32\n",
    "    \n",
    "    ## Add a Batch Normalization layer  \n",
    "    layers.BatchNormalization(),  \n",
    "    \n",
    "    ## Add a layer of Leaky ReLU activation function with alpha being 0.2, \n",
    "    layers.LeakyReLU(alpha=0.2), \n",
    "       \n",
    "    ## Build a transposed convolution layer WITHOUT bias;\n",
    "    ##   which has a number of filters \n",
    "    ##      that would make the output, i.e. the generated image, \n",
    "    ##      to have only 1 feature dimension, i.e. 1 color channel\n",
    "    ##   3 X 3 as the kernel size; \n",
    "    ##   SET stride step as a PROPER number \n",
    "    ##      that would make the output, i.e. the generated image, \n",
    "    ##      to have its first 2 dimensions (i.e. height and width), same as the loaded mnist real image\n",
    "    ##      i.e. the generated image in same image size as the real image.        \n",
    "    ##   padding with zeros evenly to the surroundings of the input;\n",
    "    ##   and CHOOSE an activation function that could make \n",
    "    ##      the range of output fake image's pixel values the SAME as preprocessed real iamge \n",
    "    ##      i.e. a range in [-1, 1]  \n",
    "    layers.Conv2DTranspose(1, (4, 4), strides=(1, 1), padding='valid', use_bias=False, activation='tanh'),\n",
    "    ##-Q11: What are the code that you have put in place of the 1st,2nd,3rd and 6th ??? in the above code line \n",
    "    ##     to fully fulfill its purpose of design (requirements in its above comments)? \n",
    " \n",
    "    ## A following Reshape layer is to make sure the output fake image has same dimensions as the real images.\n",
    "    ##   - if error occurs (dimension not match) it may suggest you to check if the previous layer configuration (e.g. strides) is correct\n",
    "    layers.Reshape([28, 28, 1])\n",
    "])\n",
    "\n",
    "## suggestion: take a look at your model's architecture by printing it out \n",
    "##   and think it through about how the transposed convolution generate the final image through upsampling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe494594",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab31d56",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab949d33",
   "metadata": {},
   "source": [
    "### Build the Discriminator Model\n",
    "\n",
    "\n",
    "This Discriminator Model in DCGAN is basically a normal **CNN*** (So you could **recall how you build your CNN model in previous assignment**) that can be trained for binary classification, to identify real or fake images. \n",
    "\n",
    "The Discriminator Model will receive images as inputs, and binary values 1 or 0 as the outputs, representing the classification of the input images to be real or fake. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cfff2c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing a keras binary activation functiono\n",
    "from keras import backend as K\n",
    "\n",
    "def binary_activation(x):\n",
    "    # Threshold the input and return binary values\n",
    "    return K.cast(K.greater(x, 0.0), K.floatx())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "97862e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the input dimension for Discriminator Model\n",
    "##   to the image shape in the tensor format (image_height, image_width, color_channels)\n",
    "image_dim = (28, 28, 1)\n",
    "\n",
    "\n",
    "# Build a tf.keras.Sequential model by stacking layers\n",
    "\n",
    "discriminator = keras.Sequential([      \n",
    "    ## Build a Convolutional layer that \n",
    "    ##   `input_shape` equal to the image dimention,  \n",
    "    ##   has 64 filters, \n",
    "    ##   5 X 5 as the kernel size, \n",
    "    ##   stride step as 2,and \n",
    "    ##   padding with zeros evenly to the surroundings of the input.     \n",
    "    layers.Conv2D(28*28, (5, 5), strides=(2, 2), input_shape=image_dim, padding='valid'),\n",
    "    \n",
    "    ##-Q12: What is the code line that you should put in place of the ??? line in the above  \n",
    "    ##     to fully fulfill its purpose of design (requirements in its above comments)? \n",
    "    ##   - use exactly only one space when needed, use only exactly one space after comma when comma is needed\n",
    "       \n",
    "    \n",
    "    ## Add a layer of Leaky ReLU activation function with alpha being 0.2, \n",
    "    layers.LeakyReLU(alpha=0.2), \n",
    "    \n",
    "    ## Add a Droupout layer to reduce overfitting     \n",
    "    ##   and set the rate to be 0.3 of the input units for dropping out  \n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    \n",
    "    ## Build a Convolutional layer that   \n",
    "    ##   has 128 filters, \n",
    "    ##   3 X 3 as the kernel size, \n",
    "    ##   stride step as 2,and \n",
    "    ##   padding with zeros evenly to the surroundings of the input.\n",
    "    layers.Conv2D(128, (3, 3), strides=(2, 2), padding='valid'),\n",
    "\n",
    "    ## Add a layer of Leaky ReLU activation function with alpha being 0.2, \n",
    "    layers.LeakyReLU(alpha=0.2), \n",
    "    \n",
    "    ## Add a Droupout layer to reduce overfitting     \n",
    "    ##   and set the rate to be 0.3 of the input units for dropping out  \n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    \n",
    "    ## Build a Convolutional layer that   \n",
    "    ##   has 256 filters, \n",
    "    ##   3 X 3 as the kernel size, \n",
    "    ##   stride step as 2,and \n",
    "    ##   padding with zeros evenly to the surroundings of the input.\n",
    "    ## Add a layer of Leaky ReLU activation function with alpha being 0.2, \n",
    "    ## Add a Droupout layer to reduce overfitting     \n",
    "    ##   and set the rate to be 0.3 of the input units for dropping out  \n",
    "    layers.Conv2D(256, (3, 3), strides=(2, 2), padding='valid'),\n",
    "    layers.LeakyReLU(alpha=0.2), \n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    \n",
    "    ## Biuld A Flatten layer to flatten the input  \n",
    "    layers.Flatten(),\n",
    "    ##-Q13: What is the code line that you should put in place of the ??? line in the above  \n",
    "    ##     to fully fulfill its purpose of design (requirements in its above comments)? \n",
    "    ##   - use exactly only one space when needed, use only exactly one space after comma when comma is needed\n",
    "       \n",
    "    \n",
    "    ## Apply a `full-connected` output layer \n",
    "    ##   with a unit number that can output a binary value;\n",
    "    ##   - choose a proper activation function \n",
    "    ##      for this goal of *binary output* (binary classification task)    \n",
    "    layers.Dense(units=10, activation=binary_activation)    \n",
    "    ##-Q14: What are the code that you should put in place of the ??? in the above code line \n",
    "    ##     to fully fulfill its purpose of design (requirements in its above comments)? \n",
    " \n",
    "])\n",
    "\n",
    "\n",
    "## suggestion: take a look at your model's architecture by printing it out \n",
    "##   and think it through as well as compare with your previous CNN model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436309a4",
   "metadata": {},
   "source": [
    "##### Compile Discriminator Model first \n",
    "\n",
    "Before the model is ready for training, it needs a few more settings. These are to be added during the [model's compiling](https://www.tensorflow.org/api_docs/python/tf/keras/Model) step:\n",
    "\n",
    "- [Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) : This is how the model is updated based on the data it sees and its loss function.\n",
    "- [Loss function](https://www.tensorflow.org/api_docs/python/tf/keras/losses) : This measures how accurate the model is during training. You want to minimize this function to \"steer\" the model in the right direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1460646e",
   "metadata": {},
   "source": [
    "Choose your optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "24ae3e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set your optimizer for compiling the models later\n",
    "## Select an optimizer that implements the Adam algorithm, \n",
    "##   and validate to decide a proper learning rate for your model \n",
    "my_optimizer = optimizers.Adam()\n",
    "\n",
    "##-Q15: What are the code line that you should put in place of the ??? in the above code line \n",
    "##     to fully fulfill its purpose of design (requirements in its above comments)? \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d621411c",
   "metadata": {},
   "source": [
    "Compile Discriminator Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f4376fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compile the Discriminator model with your chosen optimizer, \n",
    "##   and use Binary cross-entropy as losses, \n",
    "discriminator.compile(optimizer=my_optimizer,\n",
    "        loss=keras.losses.BinaryCrossentropy\n",
    "       )                     \n",
    "\n",
    "##-Q16: What are the code that you should put in place of the ??? in the above code line \n",
    "##     to fully fulfill its purpose of design (requirements in its above comments)? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7f295b",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52af461c",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6d8b2d",
   "metadata": {},
   "source": [
    " \n",
    "### Build the DCGAN Model\n",
    "\n",
    "\n",
    "Build the DCGAN by concatenating the Generator and Discriminator into one sequential.\n",
    "\n",
    "The DCGAN Model will receive noise vectors as inputs, and a binary values 1 or 0 as the outputs, representing the classification of the input images to be real or fake.\n",
    "\n",
    "The Generator model is only concerned with the discriminator’s performance on fake examples. Therefore, we will mark all of the layers in the Discriminator as not trainable when it is part of the GAN model so that they can not be updated and overtrained on fake examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "207d65cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## make discriminator not-trainable as of now\n",
    "discriminator.trainable = False\n",
    "\n",
    "## Combine both Generator and Discriminator Models\n",
    "dcgan = keras.Sequential([     \n",
    "    generator,   \n",
    "    discriminator      \n",
    "])\n",
    "\n",
    "\n",
    "##-Q17: What is the `total number` of params (i.e. weights) , \n",
    "##      including the Non-trainable params e.g. from the Discriminator part, \n",
    "##      in the whole DCGAN Model build above?\n",
    "##      be careful in the calculations, For example. with or without bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4487da",
   "metadata": {},
   "source": [
    "##### Compile DCGAN  \n",
    "\n",
    "And then compile the whole DCGAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "17acb31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compile the DCGAN model as a whole with your chosen optimizer, \n",
    "##   and use Binary cross-entropy as losses, \n",
    "dcgan.compile(optimizer=optimizers.Adam(),\n",
    "              loss=keras.losses.BinaryCrossentropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4352b2f8",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db42b50d",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6c1bc7",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eaa705",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Train the DCGAN to improve the ability of generating good fake images by the Generator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6813c8d4",
   "metadata": {},
   "source": [
    "Prepare 6 random noice vectors, to check and compare how the generated fake images on same 6 inputs are improved over the training procedure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ae22c060",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 15:55:10.367611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-06-02 15:55:10.530528: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## create random noice inputs to checking the corresponding generated fake images \n",
    "\n",
    "## create random noise inputs of 6\n",
    "##    which will be later used for comparison of the generated images\n",
    "##    on these same sample noises during the training \n",
    "noise_inputs_check = get_noise_one_batch(6)  \n",
    "## generate the initial output images before training\n",
    "images_outputs_check = generator.predict(noise_inputs_check)\n",
    "\n",
    "## Show the 6 generated images: \n",
    "images_show(images_outputs_check, num_postfix=-1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3d10ac",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3668487a",
   "metadata": {},
   "source": [
    "##### Train Discriminator Model and DCGAN Model  \n",
    "\n",
    "In each training loop, the Discriminator Model is first trained to better classify real images (drawn from the real image dataset ) and fakes images (produced by the Generator Model based on random noice as input). \n",
    "\n",
    "The Discriminator in DCGAN was first marked as not trainable to train only weights of layers in the Generator of DCGAN Model to help better generate fake images like real. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "39f9023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define training parameters  \n",
    "\n",
    "## set epochs size/count \n",
    "##  it is assigned a initial value 1 TEMPORARILY, you need to replace it to your tunned value eventually.\n",
    "EPOCHS = 1   ### 1 is a temporary value that you need to update before submitting \n",
    "\n",
    "\n",
    "## Calculated the Number of batches based on the loaded data `images_train`  and the size of a batch\n",
    "BATCHS = np.floor(BUFFER_SIZE/BATCH_SIZE).astype('int')\n",
    "\n",
    "## Set the interval as 300 \n",
    "##   to show generated images by the Generator trained so far in the training procedure\n",
    "##   on the same 6 noise vector inputs.\n",
    "##   i.e. Display every 300 iterations the 6 generated fake images as training keeps improving it\n",
    "iter_interval = 300\n",
    "\n",
    "## Define binary labels that represent real and fake outputs for each batch training\n",
    "labels_real = np.ones((BATCH_SIZE, 1))\n",
    "labels_fake = np.zeros((BATCH_SIZE, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb82a225",
   "metadata": {},
   "source": [
    "\n",
    "###  <font color='red'>NOTICE:</font>\n",
    " \n",
    "<font color='red'>Depending on your EPOCHS count, your computer computation capability, etc. the training procedure could take HOURS </font>\n",
    "\n",
    "You could temporarily replace the variable name `EPOCHS` in the below code cell to be a small value for testing.  After tunning all other parameters above this cell, you can then replace that small value back to the original variable name `EPOCHS`, so your setting on the `EPOCHS` in the above code cell could take effect.\n",
    "\n",
    "For example, if you replace　　`for epoch in range(0, EPOCHS):`　　to be　 　`for epoch in range(0, 1):`　　\n",
    "     \n",
    "and execute the below code cell, the DCGAN will only be trained for 1 epoch for all the batches, and stop there.\n",
    "\n",
    "<br>\n",
    "\n",
    "After that you can keep manually changing it to be 　　`for epoch in range(1, 2):`\n",
    "     \n",
    "and execute the below code cell again (without needing to restart the whole notebook), the DCGAN will keep beeing trained for the 2nd epoch, and the training will continue based on the parameters already learned from the 1st epoch.\n",
    "\n",
    "As long as your computer, Jupyter notebook local server, and this .ipynb, are not colsed, you should be able to keep on the training procesure, instead of training all over again. \n",
    "\n",
    "For example if you want to run the 3rd epoch, then keep manually change it to be 　　 `for epoch in range(2, 3):` \n",
    "\n",
    "then the model will continued to be trained upon the parameters learned from the 2nd epoch\n",
    "\n",
    "<br>\n",
    "     \n",
    "This way of manually start the next epoch round could let you have more controls on the tunning or debugging, and have a better idea how many epochs your model would eventually need. 　Note re-run the below cell will overwrite the output so you won't be able to see the image plots generated from previous epoch, but you can always go into your image subfolder to inspect.\n",
    "\n",
    "After you tunned to decide a satisfied epoch count (as well as other parameters), then you can assign that value to the variable `EPOCHS` in the above code cell, and change the bellow cell back to `for epoch in range(0, EPOCHS):`, and save the .ipynb before colse and halt it, then reopen the .ipynb to have a full run-from-beginning, to complete this assignment for the submission.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e4e7af1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~ DCGAN training starting at 2023-06-02 15:55:12.778473 \n",
      "\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 15:55:12.871608: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [60000,28,28,1]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-06-02 15:55:12.871788: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [60000,28,28,1]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-06-02 15:55:12.910346: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [64,1]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/home/ceg98/miniconda3/envs/ML/lib/python3.9/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/ceg98/miniconda3/envs/ML/lib/python3.9/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/ceg98/miniconda3/envs/ML/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/ceg98/miniconda3/envs/ML/lib/python3.9/site-packages/keras/engine/training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/ceg98/miniconda3/envs/ML/lib/python3.9/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/home/ceg98/miniconda3/envs/ML/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/ceg98/miniconda3/envs/ML/lib/python3.9/site-packages/keras/losses.py\", line 160, in __call__\n        return losses_utils.compute_weighted_loss(\n    File \"/home/ceg98/miniconda3/envs/ML/lib/python3.9/site-packages/keras/utils/losses_utils.py\", line 328, in compute_weighted_loss\n        losses = tf.convert_to_tensor(losses)\n\n    TypeError: Failed to convert elements of <keras.losses.BinaryCrossentropy object at 0x7fa99cc02820> to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m discriminator\u001b[39m.\u001b[39mtrainable \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m## Batch Train Discriminator Model on a batch of real images with them labeled as real \u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m d_loss_real \u001b[39m=\u001b[39m discriminator\u001b[39m.\u001b[39;49mtrain_on_batch(images_real, labels_real)\n\u001b[1;32m     22\u001b[0m \u001b[39m## create a random noise batch inputs and pass which into the Generator to generate fake images\u001b[39;00m\n\u001b[1;32m     23\u001b[0m noise_inputs \u001b[39m=\u001b[39m get_noise_one_batch()\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.9/site-packages/keras/engine/training.py:2510\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   2506\u001b[0m     iterator \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39msingle_batch_iterator(\n\u001b[1;32m   2507\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy, x, y, sample_weight, class_weight\n\u001b[1;32m   2508\u001b[0m     )\n\u001b[1;32m   2509\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_train_function()\n\u001b[0;32m-> 2510\u001b[0m     logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   2512\u001b[0m logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   2513\u001b[0m \u001b[39mif\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileinxfc1zx.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.9/site-packages/keras/engine/training.py:1268\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m   1264\u001b[0m     run_step \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mfunction(\n\u001b[1;32m   1265\u001b[0m         run_step, jit_compile\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, reduce_retracing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1266\u001b[0m     )\n\u001b[1;32m   1267\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(iterator)\n\u001b[0;32m-> 1268\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdistribute_strategy\u001b[39m.\u001b[39;49mrun(run_step, args\u001b[39m=\u001b[39;49m(data,))\n\u001b[1;32m   1269\u001b[0m outputs \u001b[39m=\u001b[39m reduce_per_replica(\n\u001b[1;32m   1270\u001b[0m     outputs,\n\u001b[1;32m   1271\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy,\n\u001b[1;32m   1272\u001b[0m     reduction\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_reduction_method,\n\u001b[1;32m   1273\u001b[0m )\n\u001b[1;32m   1274\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.9/site-packages/keras/engine/training.py:1249\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_step\u001b[39m(data):\n\u001b[0;32m-> 1249\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[1;32m   1250\u001b[0m     \u001b[39m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.9/site-packages/keras/engine/training.py:1051\u001b[0m, in \u001b[0;36mModel.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m   1050\u001b[0m     y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(x, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m-> 1051\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss(x, y, y_pred, sample_weight)\n\u001b[1;32m   1052\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_target_and_loss(y, loss)\n\u001b[1;32m   1053\u001b[0m \u001b[39m# Run backwards pass.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.9/site-packages/keras/engine/training.py:1109\u001b[0m, in \u001b[0;36mModel.compute_loss\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute the total loss, validate it, and return it.\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \n\u001b[1;32m   1060\u001b[0m \u001b[39mSubclasses can optionally override this method to provide custom loss\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m  is the case when called by `Model.test_step`).\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mdel\u001b[39;00m x  \u001b[39m# The default implementation does not use `x`.\u001b[39;00m\n\u001b[0;32m-> 1109\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompiled_loss(\n\u001b[1;32m   1110\u001b[0m     y, y_pred, sample_weight, regularization_losses\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlosses\n\u001b[1;32m   1111\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.9/site-packages/keras/engine/compile_utils.py:265\u001b[0m, in \u001b[0;36mLossesContainer.__call__\u001b[0;34m(self, y_true, y_pred, sample_weight, regularization_losses)\u001b[0m\n\u001b[1;32m    263\u001b[0m y_t, y_p, sw \u001b[39m=\u001b[39m match_dtype_and_rank(y_t, y_p, sw)\n\u001b[1;32m    264\u001b[0m sw \u001b[39m=\u001b[39m losses_utils\u001b[39m.\u001b[39mapply_mask(y_p, sw, losses_utils\u001b[39m.\u001b[39mget_mask(y_p))\n\u001b[0;32m--> 265\u001b[0m loss_value \u001b[39m=\u001b[39m loss_obj(y_t, y_p, sample_weight\u001b[39m=\u001b[39;49msw)\n\u001b[1;32m    267\u001b[0m total_loss_mean_value \u001b[39m=\u001b[39m loss_value\n\u001b[1;32m    268\u001b[0m \u001b[39m# Correct for the `Mean` loss metrics counting each replica as a\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[39m# batch.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.9/site-packages/keras/losses.py:160\u001b[0m, in \u001b[0;36mLoss.__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    156\u001b[0m reduction \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_reduction()\n\u001b[1;32m    157\u001b[0m sample_weight \u001b[39m=\u001b[39m losses_utils\u001b[39m.\u001b[39mapply_valid_mask(\n\u001b[1;32m    158\u001b[0m     losses, sample_weight, mask, reduction\n\u001b[1;32m    159\u001b[0m )\n\u001b[0;32m--> 160\u001b[0m \u001b[39mreturn\u001b[39;00m losses_utils\u001b[39m.\u001b[39mcompute_weighted_loss(\n\u001b[1;32m    161\u001b[0m     losses, sample_weight, reduction\u001b[39m=\u001b[39mreduction\n\u001b[1;32m    162\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.9/site-packages/keras/utils/losses_utils.py:328\u001b[0m, in \u001b[0;36mcompute_weighted_loss\u001b[0;34m(losses, sample_weight, reduction, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mget_default_graph()\u001b[39m.\u001b[39m_last_loss_reduction \u001b[39m=\u001b[39m reduction\n\u001b[1;32m    327\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(losses, (keras_tensor\u001b[39m.\u001b[39mKerasTensor, tf\u001b[39m.\u001b[39mRaggedTensor)):\n\u001b[0;32m--> 328\u001b[0m     losses \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(losses)\n\u001b[1;32m    330\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m    331\u001b[0m     sample_weight, (keras_tensor\u001b[39m.\u001b[39mKerasTensor, tf\u001b[39m.\u001b[39mRaggedTensor)\n\u001b[1;32m    332\u001b[0m ):\n\u001b[1;32m    333\u001b[0m     sample_weight \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(sample_weight)\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/home/ceg98/miniconda3/envs/ML/lib/python3.9/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/ceg98/miniconda3/envs/ML/lib/python3.9/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/ceg98/miniconda3/envs/ML/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/ceg98/miniconda3/envs/ML/lib/python3.9/site-packages/keras/engine/training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/ceg98/miniconda3/envs/ML/lib/python3.9/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/home/ceg98/miniconda3/envs/ML/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/ceg98/miniconda3/envs/ML/lib/python3.9/site-packages/keras/losses.py\", line 160, in __call__\n        return losses_utils.compute_weighted_loss(\n    File \"/home/ceg98/miniconda3/envs/ML/lib/python3.9/site-packages/keras/utils/losses_utils.py\", line 328, in compute_weighted_loss\n        losses = tf.convert_to_tensor(losses)\n\n    TypeError: Failed to convert elements of <keras.losses.BinaryCrossentropy object at 0x7fa99cc02820> to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\n"
     ]
    }
   ],
   "source": [
    "## Start Model Training in the iterations\n",
    "\n",
    "## Record the Starting time of runing DCGAN training\n",
    "datetime_now = datetime.now()\n",
    "print(f\"~~~ DCGAN training starting at {datetime_now} \\n\\n \")  \n",
    "\n",
    "for epoch in range(0, EPOCHS): \n",
    "    ## generate new dataset of real images for each epoch\n",
    "    real_dataset = get_dataset_real(images_data)\n",
    "    \n",
    "    for batch, images_real in enumerate(real_dataset):\n",
    "        \n",
    "        ## Train Discriminator seperately:\n",
    "        \n",
    "        ## Make Discriminator Trainable for now\n",
    "        ##   i.e.set the Discriminator Model trainable to be trained on real and fake images\n",
    "        discriminator.trainable = True\n",
    "        \n",
    "        ## Batch Train Discriminator Model on a batch of real images with them labeled as real \n",
    "        d_loss_real = discriminator.train_on_batch(images_real, labels_real)\n",
    "        \n",
    "        ## create a random noise batch inputs and pass which into the Generator to generate fake images\n",
    "        noise_inputs = get_noise_one_batch()\n",
    "        images_fake = generator.predict(noise_inputs)\n",
    "                          \n",
    "        ## Train Discriminator Model on a batch of fake images with them labeled as fake \n",
    "        d_loss_fake = discriminator.train_on_batch(images_fake, labels_fake)\n",
    "                          \n",
    "            \n",
    "        ## Train DCGAN model     \n",
    "        \n",
    "        ## Make Discriminator Not-trainable for now\n",
    "        ##   i.e. set the Discriminator Model NOT trainable \n",
    "        ##        to only update layer weights of Generator            \n",
    "        discriminator.trainable = False\n",
    "        ##-Q18: What is the code line that you should put in place of the ??? in the above line \n",
    "        ##     to fully fulfill its purpose of design (requirements in its above comments)? \n",
    "        \n",
    "        ## create a random noise batch inputs and pass which into the DCGAN later\n",
    "        noise_inputs = get_noise_one_batch()\n",
    "        \n",
    "        ## Batch Train DCGAN Model on a batch of noise inputs with them labeled as real \n",
    "        ##   to fool the Discriminator with the fake images generated by Generator but labeled as real\n",
    "        dcg_loss = dcgan.train_on_batch(noise_inputs, labels_real) \n",
    "        ##-Q19: What is the code line that you should put in place of the ??? in the above line \n",
    "        ##     to fully fulfill its purpose of design (requirements in its above comments)? \n",
    "    \n",
    "        ## Print training progress with model losses  on the intervals  \n",
    "        ##   and Show fake images generated on same 6 noise inputs \n",
    "        ##            which are also saved in the folder for comparison later\n",
    "        iteration = (epoch * BATCHS) + (batch + 1)  \n",
    "\n",
    "        if iteration % 5 == 0:\n",
    "            print(f\"=== Iter: {iteration} : d_loss_real: {d_loss_real:.9f}, d_loss_fake: {d_loss_fake:.9f}, dcg_loss: {dcg_loss:.9f} - in Epoch {epoch + 1}/{EPOCHS} - Batch {batch+1}/{BATCHS}\")\n",
    "\n",
    "        if iteration % iter_interval == 0: \n",
    "            images_check = generator.predict(noise_inputs_check)\n",
    "            images_show(images_check, num_postfix=iteration)\n",
    "            print(\"*** time now: \", datetime.now()) \n",
    "            \n",
    "## Record the Ending time of runing DCGAN training\n",
    "datetime_now = datetime.now()\n",
    "print(f\"\\n\\n ~~~ DCGAN trainging ends at {datetime_now}\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
